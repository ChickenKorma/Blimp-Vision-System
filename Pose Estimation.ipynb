{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Estimation Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras_tuner\n",
    "import keras.backend as K\n",
    "import math as m\n",
    "import time\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.set_printoptions(suppress = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_prefix = \"D:/Uni Stuff/IP/Data/\"\n",
    "image_path_prefix = data_path_prefix + \"Blimp Images/Raw/image_\"\n",
    "\n",
    "blimp_vertices = np.array([[1, 1, 1], [1, 1, -1], [1, -1, 1], [1, -1, -1], [-1, 1, 1], [-1, 1, -1], [-1, -1, -1], [-1, -1, 1]])\n",
    "\n",
    "test_split = 0.25\n",
    "\n",
    "desired_image_size = 16\n",
    "\n",
    "image_columns = []\n",
    "colours = {0:\"R\", 1:\"G\", 2:\"B\"}\n",
    "\n",
    "# Create list of column names like '0 0 R', '0 0 G', '0 0 B', '0 1 R' etc\n",
    "for i in range(desired_image_size):\n",
    "    for j in range(desired_image_size):\n",
    "        for k in range(3):\n",
    "            image_columns.append(str(i) + \" \" + str(j) + \" \" + colours[k])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSVs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the bounding box and pose csv files and add to dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_data = pd.read_csv(data_path_prefix + \"bbox data.csv\")\n",
    "\n",
    "pose_data = pd.read_csv(data_path_prefix + \"blimp poses.csv\")\n",
    "y_variables = list(pose_data.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the images from file, crop these by the bounding box and then resize to the desired square image size with padding to preserve aspect ratio. Image data is flattened and added to dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop the image to the bounding box coordinates\n",
    "def cutout_image(image, bbox):\n",
    "    width = image.shape[1]\n",
    "    height = image.shape[0]\n",
    "\n",
    "    center_x = bbox['cent_x'] * width\n",
    "    center_y = bbox['cent_y'] * height\n",
    "\n",
    "    bbox_width = bbox['width'] * width\n",
    "    bbox_height = bbox['height'] * height\n",
    "\n",
    "    min_x = np.clip(int(center_x - (bbox_width / 2)), 0, width)\n",
    "    max_x = np.clip(int(center_x + (bbox_width / 2)), 0, width) \n",
    "\n",
    "    min_y = np.clip(int(center_y - (bbox_height / 2)), 0, height)\n",
    "    max_y = np.clip(int(center_y + (bbox_height / 2)), 0, height)\n",
    "\n",
    "    return image[min_y:max_y, min_x:max_x]\n",
    "\n",
    "# Resize the image to the desired_image_size square and pad with black if necessary\n",
    "def resize_and_pad_image(image):\n",
    "    old_size = image.shape[:2]\n",
    "\n",
    "    size_ratio = float(desired_image_size) / max(old_size)\n",
    "\n",
    "    new_size = tuple([int(x * size_ratio) for x in old_size])\n",
    "\n",
    "    resized_image = cv.resize(image, (new_size[1], new_size[0]), interpolation=cv.INTER_AREA)\n",
    "\n",
    "    delta_w = desired_image_size - new_size[1]\n",
    "    delta_h = desired_image_size - new_size[0]\n",
    "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "\n",
    "    return cv.copyMakeBorder(resized_image, top, bottom, left, right, cv.BORDER_CONSTANT, value=[0, 0, 0])\n",
    "\n",
    "for i in range(len(pose_data)):\n",
    "    img = cv.imread(image_path_prefix + str(i) + \".png\")\n",
    "    cropped = cutout_image(img, bbox_data.iloc[i])\n",
    "    padded = resize_and_pad_image(cropped)\n",
    "\n",
    "    cv.imwrite(data_path_prefix + \"Processed Images/image_\" + str(i) + \".png\", padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = data_path_prefix + \"Processed Images/image_\"\n",
    "\n",
    "save_path = data_path_prefix + \"Pose Estimation Images/\"\n",
    "train_save_path = save_path + \"train/\"\n",
    "test_save_path = save_path + \"test/\"\n",
    "\n",
    "X_train_nums, X_test_nums, y_train, y_test = train_test_split(np.arange(15000), pose_data, test_size=0.15)\n",
    "\n",
    "for num in X_train_nums:\n",
    "    image = cv.imread(load_path + str(num) + \".png\")\n",
    "    cv.imwrite(train_save_path + \"image_\" + str(num) + \".png\", image)\n",
    "\n",
    "print(\"Train images done\")\n",
    "\n",
    "for num in X_test_nums:\n",
    "    image = cv.imread(load_path + str(num) + \".png\")\n",
    "    cv.imwrite(test_save_path + \"image_\" + str(num) + \".png\", image)\n",
    "\n",
    "print(\"Test images done\")\n",
    "\n",
    "y_train.to_csv(train_save_path + \"data.csv\")\n",
    "y_test.to_csv(test_save_path + \"data.csv\")\n",
    "\n",
    "print(\"Data done\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove unecessary variables from RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bbox_data\n",
    "del pose_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADD Metric"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create functions to calculate ADD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the rotation matrix of an XYZ rotation with the euler angles vector\n",
    "def get_rotation_matrix(rot_vec):\n",
    "    x_matrix = np.matrix([[ 1, 0, 0],\n",
    "                          [ 0, m.cos(m.radians(rot_vec[0])),-m.sin(m.radians(rot_vec[0]))],                         \n",
    "                          [ 0, m.sin(m.radians(rot_vec[0])), m.cos(m.radians(rot_vec[0]))]])\n",
    "    \n",
    "    y_matrix = np.matrix([[ m.cos(m.radians(rot_vec[1])), 0, m.sin(m.radians(rot_vec[1]))],\n",
    "                          [ 0, 1, 0],\n",
    "                          [-m.sin(m.radians(rot_vec[1])), 0, m.cos(m.radians(rot_vec[1]))]])\n",
    "    \n",
    "    z_matrix = np.matrix([[ m.cos(m.radians(rot_vec[2])), -m.sin(m.radians(rot_vec[2])), 0 ],\n",
    "                          [ m.sin(m.radians(rot_vec[2])),  m.cos(m.radians(rot_vec[2])), 0 ],\n",
    "                          [ 0, 0, 1 ]])\n",
    "    \n",
    "    return z_matrix * y_matrix * x_matrix\n",
    "\n",
    "# Get the average distance between true and predicted model points\n",
    "def get_ADD(true_trans, true_rot_vec, pred_trans, pred_rot_vec):\n",
    "    true_rot = get_rotation_matrix(true_rot_vec).T\n",
    "    pred_rot = get_rotation_matrix(pred_rot_vec).T\n",
    "\n",
    "    total_distance = 0\n",
    "\n",
    "    for vertex_pos in blimp_vertices:\n",
    "        true_pos = np.matmul(true_rot, vertex_pos) - true_trans\n",
    "        pred_pos = np.matmul(pred_rot, vertex_pos) - pred_trans\n",
    "\n",
    "        distance = np.linalg.norm(true_pos - pred_pos)\n",
    "        total_distance += distance\n",
    "\n",
    "    return total_distance / len(blimp_vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADD_metric(y_true, y_pred):\n",
    "    true_trans = y_true[:3]\n",
    "    true_rot_vec = y_true[3:]\n",
    "\n",
    "    pred_trans = y_pred[:3]\n",
    "    pred_rot_vec = y_pred[3:]\n",
    "\n",
    "    #print(K.eval(true_trans))\n",
    "\n",
    "    try: \n",
    "        true_trans = K.eval(true_trans)\n",
    "        true_rot_vec = K.eval(true_rot_vec)\n",
    "        pred_trans = K.eval(pred_trans)\n",
    "        pred_rot_vec = K.eval(pred_rot_vec)\n",
    "    except:\n",
    "        return 10000\n",
    "\n",
    "    return get_ADD(true_trans, true_rot_vec, pred_trans, pred_rot_vec)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_path = data_path_prefix + \"Pose Estimation Data/train/images/\"\n",
    "test_images_path = data_path_prefix + \"Pose Estimation Data/test/images/\"\n",
    "\n",
    "X_train = pd.DataFrame(columns=image_columns)\n",
    "X_test = pd.DataFrame(columns=image_columns)\n",
    "\n",
    "for image_name in os.listdir(train_images_path):\n",
    "    image = cv.imread(train_images_path + image_name)\n",
    "    X_train.loc[len(X_train)] = image.flatten()\n",
    "\n",
    "for image_name in os.listdir(test_images_path):\n",
    "    image = cv.imread(test_images_path + image_name)\n",
    "    X_test.loc[len(X_test)] = image.flatten()\n",
    "\n",
    "y_train = pd.read_csv(data_path_prefix + \"Pose Estimation Data/train/data.csv\")\n",
    "y_test = pd.read_csv(data_path_prefix + \"Pose Estimation Data/test/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.drop(y_train.columns[0], axis=1)\n",
    "y_test = y_test.drop(y_test.columns[0], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create normalizer function based on input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(np.array(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the DNN model and print details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_model(hp):\n",
    "  #NAME = f\"{dense_layers}x{nodes}-{activation}-{optimizer.name}({optimizer.learning_rate})-{int(time.time())}\"\n",
    "  #tensorboard = tf.keras.callbacks.TensorBoard(log_dir=f'logs/{NAME}')\n",
    "\n",
    "  layers = [normalizer]\n",
    "\n",
    "  for i in range(hp.Choice(\"blocks\", [1])):\n",
    "    for j in range(hp.Choice(\"layers_per_block\", [64])):\n",
    "      layers.append(tf.keras.layers.Dense(hp.Choice(\"nodes_per_layer\", [64]), activation='relu'))\n",
    "    \n",
    "    #layers.append(tf.keras.layers.MaxPooling3D(data_format=\"channels_first\"))\n",
    "\n",
    "  layers.append(tf.keras.layers.Dense(6))\n",
    "\n",
    "  model = tf.keras.Sequential(layers)\n",
    "  model.compile(loss = 'mean_absolute_error', metrics=['accuracy', 'mean_absolute_error'], optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "\n",
    "  return model\n",
    "\n",
    "#dnn_model = build_and_compile_model(normalizer, 2, 64, 'relu', tf.keras.optimizers.Adam(0.001))\n",
    "#dnn_model.summary()\n",
    "\n",
    "\n",
    "tuner = keras_tuner.RandomSearch(\n",
    "    build_and_compile_model,\n",
    "    max_trials=10,\n",
    "    # Do not resume the previous search in the same directory.\n",
    "    overwrite=True,\n",
    "    objective=\"val_loss\",\n",
    "    # Set a directory to store the intermediate results.\n",
    "    directory=\"logs/\",\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=0.215,\n",
    "    epochs=10,\n",
    "    # Use the TensorBoard callback.\n",
    "    # The logs will be write to \"/tmp/tb_logs\".\n",
    "    callbacks=[tf.keras.callbacks.TensorBoard(\"logs/\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models()[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss and epoch graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  plt.plot(history.history['val_loss'], label='val_loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Error')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "\n",
    "plot_loss(best_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate performance out-of-sample on the testing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the true and predicted poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Actual:\")\n",
    "print(y_test)\n",
    "\n",
    "print(\"Predicted:\")\n",
    "predictions = best_model.predict(X_test)\n",
    "print(pd.DataFrame(predictions, columns=[\"pos_x\", \"pos_y\", \"pos_z\", \"rot_x\", \"rot_y\", \"rot_z\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
